# Memory Retrieval 품질 평가 체계

## 개요

`MemorySummarizer`가 생성한 장기 기억이 **특정 쿼리에 유용한가**를 평가하기 위한 체계입니다.

### 평가 대상

```
사용자 쿼리 → 세션 전체 메모리 조회 → LLM 응답 생성
                    ↑
          "이 쿼리에 유용한 정보가 있는가?" 평가
```

- **입력**: 사용자 쿼리 + 저장된 메모리
- **평가 내용**: 메모리가 해당 쿼리를 처리하는 데 도움이 되는가?

### 평가 방식

- **점수 범위**: 0-10 (높을수록 좋음)
- **전체 점수**: 0-100 (높을수록 좋음)
- **평가 주체**: LLM-as-Judge

---

## 평가 기준

### 1. Relevance (관련성)

**정의**: 메모리에 쿼리와 관련된 정보가 있는가?

**점수 기준**:
| 점수 | 설명 |
|------|------|
| 9-10 | 쿼리에 직접 관련된 정보가 명확히 존재 |
| 7-8 | 관련 정보가 있으나 간접적 |
| 4-6 | 부분적으로 관련 있음 |
| 1-3 | 거의 관련 없음 |
| 0 | 전혀 관련 없음 |

**예시**:
| 쿼리 | 메모리 | 점수 |
|------|--------|------|
| "AWS 비용 얼마야?" | "사용자는 AWS 주력, 월 1.2억 지출" | 10 |
| "AWS 비용 얼마야?" | "사용자는 클라우드 비용 관심" | 5 |
| "AWS 비용 얼마야?" | "사용자가 RI 커버리지 질문함" | 2 |

---

### 2. Completeness (완전성)

**정의**: 쿼리 응답에 필요한 맥락이 메모리에 충분한가?

**점수 기준**:
| 점수 | 설명 |
|------|------|
| 9-10 | 응답에 필요한 모든 맥락 존재 |
| 7-8 | 대부분의 맥락 존재, 일부 누락 |
| 4-6 | 기본 맥락만 존재 |
| 1-3 | 맥락이 거의 없음 |
| 0 | 맥락 전혀 없음 |

**예시**:
| 쿼리 | 메모리 | 점수 |
|------|--------|------|
| "지난번 말한 예산 초과 어떻게 됐어?" | "예산 1억, 현재 1.2억 지출, 초과 우려 언급" | 10 |
| "지난번 말한 예산 초과 어떻게 됐어?" | "예산 관련 논의 있었음" | 4 |
| "지난번 말한 예산 초과 어떻게 됐어?" | "(메모리 없음)" | 0 |

---

### 3. Accuracy (정확성)

**정의**: 메모리의 정보가 쿼리 처리에 정확하게 활용될 수 있는가?

**점수 기준**:
| 점수 | 설명 |
|------|------|
| 9-10 | 정보가 정확하고 신뢰 가능 |
| 7-8 | 대체로 정확, 사소한 오차 |
| 4-6 | 일부 부정확한 정보 혼재 |
| 1-3 | 상당 부분 부정확 |
| 0 | 정보가 완전히 틀림 |

**부정확 유형**:
- **수치 오류**: 금액, 비율 등 숫자 오기
- **시점 오류**: 과거/현재 혼동
- **대상 오류**: AWS↔GCP 등 혼동
- **상태 오류**: 완료↔진행중 혼동

---

### 4. Noise (노이즈)

**정의**: 무관한 정보가 쿼리 처리를 방해하는가?

**점수 기준** (역방향 - 낮을수록 좋음):
| 점수 | 설명 |
|------|------|
| 0-1 | 노이즈 거의 없음, 깔끔함 |
| 2-3 | 약간의 무관 정보 |
| 4-6 | 노이즈가 있으나 핵심 정보 식별 가능 |
| 7-8 | 노이즈가 많아 핵심 찾기 어려움 |
| 9-10 | 노이즈가 심해 쿼리 처리 방해 |

**노이즈 유형**:
- 이전에 해결된 이슈
- 현재 쿼리와 무관한 주제
- 일반적인 시스템 안내 문구
- 오래된/갱신되지 않은 정보

---

## 전체 점수 계산

```
overall_score = (relevance * 0.35) + (completeness * 0.30) + (accuracy * 0.25) + ((10 - noise) * 0.10)
overall_score = overall_score * 10  # 0-100 스케일
```

**가중치 설명**:
- Relevance (35%): 가장 중요 - 관련 정보가 없으면 무용
- Completeness (30%): 맥락이 있어야 정확한 응답 가능
- Accuracy (25%): 잘못된 정보는 잘못된 응답 유발
- Noise (10%): 방해 요소이나 다른 요소보다 덜 치명적

---

## 평가 프롬프트 템플릿

```
당신은 AI 메모리 시스템 평가자입니다.

## 평가 대상
- 사용자 쿼리: {query}
- 저장된 메모리: {memory}
- 저장된 엔티티: {entities}

## 평가 기준 (각 0-10점)
1. relevance (관련성): 쿼리와 관련된 정보가 메모리에 있는가? (높을수록 좋음)
2. completeness (완전성): 쿼리 응답에 필요한 맥락이 충분한가? (높을수록 좋음)
3. accuracy (정확성): 메모리 정보가 정확한가? (높을수록 좋음)
4. noise (노이즈): 무관한 정보가 방해되는가? (낮을수록 좋음)

## 응답 형식 (JSON)
{
    "overall_score": 0-100,
    "scores": {
        "relevance": {"score": N, "reason": "..."},
        "completeness": {"score": N, "reason": "..."},
        "accuracy": {"score": N, "reason": "..."},
        "noise": {"score": N, "reason": "..."}
    },
    "helpful_info": ["쿼리 처리에 도움되는 정보 1", "..."],
    "missing_info": ["있었으면 좋았을 정보 1", "..."],
    "summary": "한 줄 평가"
}
```

---

## 테스트 시나리오

### 1. 연속 대화 테스트
여러 턴 대화 후, 이전 대화 내용을 참조하는 쿼리로 평가

```
Turn 1: "AWS 비용이 1.2억이에요"
Turn 2: "예산은 1억이에요"
Turn 3: "RI 커버리지는 60%예요"
...
Turn 10: "아까 말한 예산 초과 상황 다시 설명해줘"  ← 평가 쿼리
```

### 2. 주제 전환 테스트
주제가 바뀐 후에도 이전 주제 정보가 필요한 경우

```
Turn 1-5: 비용 관련 대화
Turn 6-10: 거버넌스 관련 대화
Turn 11: "처음에 말한 AWS 비용이 얼마였지?"  ← 평가 쿼리
```

### 3. 정보 갱신 테스트
정보가 업데이트된 후 최신 정보를 참조하는 경우

```
Turn 1: "예산이 1억이에요"
Turn 5: "예산이 1.5억으로 늘었어요"
Turn 10: "현재 예산이 얼마야?"  ← 평가 쿼리 (1.5억이 나와야 함)
```

---

## 관련 파일

### 평가 대상 (요약 로직)
- `src/processors/memory_summarizer_processor.py`: `MemorySummarizer.recursive_summarize()`

### 평가 도구
- `scripts/memory_test.py`: 자동 테스트 러너 (10-20턴 대화 후 평가)
- `scripts/chat_cli.py`: `/eval [쿼리]` 명령어 (수동 평가)
  - 예: `/eval 아까 말한 예산 초과 상황 다시 설명해줘`

### 저장소
- `src/processors/memory_store_processor.py`: Redis 저장/조회